{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fd75fbbdad0>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [ to_ix[word] for word in seq ]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making training data with tags as labels of nouns, verbs, determiner\n",
    "# Tags are: DET - determiner; NN - noun; V - verb\n",
    "\n",
    "train_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx ={}\n",
    "for sent,tags in train_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "tags_to_idx = {\"DET\":0, \"NN\":1, \"V\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMDEDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing model class\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, input_sentence):\n",
    "        embeds = self.word_embedding(input_sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(input_sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(input_sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space,1)\n",
    "        return tag_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMDEDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tags_to_idx))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0500, -0.9256, -1.3714],\n",
      "        [-1.0188, -0.9739, -1.3419],\n",
      "        [-1.1330, -0.9662, -1.2126],\n",
      "        [-1.1818, -0.9763, -1.1501],\n",
      "        [-1.0766, -0.9916, -1.2439]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  inputs = prepare_sequence(train_data[0][0], word_to_idx)\n",
    "  print(model(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "    for sent, tag in train_data:\n",
    "\n",
    "        model.zero_grad()\n",
    "        input = prepare_sequence(sent, word_to_idx)\n",
    "        target = prepare_sequence(tag, tags_to_idx)\n",
    "        log_probs = model(input)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3892, -1.2426, -3.3890],\n",
      "        [-2.1082, -0.1328, -5.8464],\n",
      "        [-3.0852, -5.9469, -0.0495],\n",
      "        [-0.0499, -3.4414, -4.0961],\n",
      "        [-2.4540, -0.0929, -5.8799]]) ['The', 'dog', 'ate', 'the', 'apple']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(train_data[0][0], word_to_idx)\n",
    "    print(model(inputs), train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2.0,
  "kernelspec": {
   "name": "pycharm-efe5f914",
   "language": "python",
   "display_name": "PyCharm (test)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}